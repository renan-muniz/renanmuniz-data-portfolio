{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a38c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"host\":\"\",\n",
    "    \"user\":\"\",\n",
    "    \"password\":\".\",\n",
    "    \"port\":\"\",\n",
    "    \"dbname\":\"\"\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3aa271a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectados\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "try:\n",
    "    conn = psycopg2.connect(**config)\n",
    "    print(\"Conectados\")\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(\"No conectados\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0807f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def bbdd(pregunta, respuesta):\n",
    "    conn = psycopg2.connect(**config)\n",
    "    cursor = conn.cursor()\n",
    "    query = \"INSERT INTO preguntas_respuestas(preguntas, respuestas, fechas) VALUES (%s, %s, %s)\"\n",
    "    cursor.execute(query, (pregunta, respuesta, datetime.now()))\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return \"ok\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea165ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "pregunta = 'test'\n",
    "respuesta = 'ok'\n",
    "\n",
    "resp = bbdd(pregunta, respuesta)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520d7048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KEY_GROQ'] = \"API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de75f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "def llm(pregunta):\n",
    "    client = Groq(\n",
    "        api_key=os.environ.get(\"KEY_GROQ\"),\n",
    "    )\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\n",
    "                    Eres un tutor de programación y Data Engineering altamente experimentado. \n",
    "                    Tu misión es ayudar al usuario a entender conceptos, resolver problemas y aprender buenas prácticas en programación. \n",
    "                    Responde únicamente preguntas relacionadas con programación, bases de datos, SQL, Python, procesamiento de datos, ETL y conceptos de Data Engineering. \n",
    "                    Si alguien pregunta sobre otro tema, indícale educadamente que solo puedes ayudar con programación y anima a que haga otra pregunta relevante.\n",
    "                    Incluye ejemplos de código cuando sea posible. \n",
    "                    Siempre que sea útil, proporciona consejos de buenas prácticas o alternativas para resolver problemas.\n",
    "                    \"\"\"\n",
    "            },\n",
    "\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\":pregunta\n",
    "            }\n",
    "        ],\n",
    "        model=\"openai/gpt-oss-20b\",\n",
    "        stream=False,\n",
    "    )\n",
    "\n",
    "    return (chat_completion.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c763f41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Que es Data Engineering?\n",
      "___________\n",
      "## ¿Qué es Data Engineering?\n",
      "\n",
      "Data Engineering (Ingeniería de Datos) es la disciplina que se encarga de diseñar, construir, mantener y optimizar los sistemas y procesos que permiten a una organización capturar, almacenar, transformar y entregar datos de forma fiable y en tiempo real. En otras palabras, los **data engineers** son los “constructores” de la infraestructura de datos que soporta a los analistas, científicos de datos y a las aplicaciones de negocio.\n",
      "\n",
      "### 1. Áreas clave\n",
      "\n",
      "| Área | Descripción | Herramientas comunes |\n",
      "|------|-------------|----------------------|\n",
      "| **Ingesta** | Recoger datos de distintas fuentes (APIs, logs, sensores, bases de datos, etc.). | Kafka, Flink, NiFi, AWS Kinesis, Google Pub/Sub |\n",
      "| **Almacenamiento** | Guardar datos en formatos estructurados y no estructurados de forma escalable. | HDFS, S3, Snowflake, BigQuery, Redshift, Databricks |\n",
      "| **Transformación** | Limpiar, enriquecer y convertir datos a un formato útil (ETL/ELT). | Spark, Airflow, dbt, Pandas, SQL |\n",
      "| **Entrega** | Exponer datos a usuarios y aplicaciones a través de APIs o vistas. | GraphQL, REST, ODBC/JDBC, Tableau, Power BI |\n",
      "| **Monitoreo** | Detectar fallos, cuellos de botella y asegurar calidad. | Prometheus, Grafana, Datadog, ELK stack |\n",
      "\n",
      "### 2. Pipeline típico de datos\n",
      "\n",
      "```\n",
      "[Fuentes] → [Ingesta] → [Almacenamiento bruto] → [Transformación] → [Almacenamiento procesado] → [Entrega]\n",
      "```\n",
      "\n",
      "- **Ingesta**: se dispara cuando llegan nuevos datos (batch o streaming).  \n",
      "- **Almacenamiento bruto**: guarda los datos “tal cual” para auditoría o reprocesamiento.  \n",
      "- **Transformación**: se aplica lógica de negocio, limpieza y enriquecimiento.  \n",
      "- **Almacenamiento procesado**: almacena los resultados listos para consumo.  \n",
      "- **Entrega**: se exponen a dashboards, modelos predictivos o APIs.\n",
      "\n",
      "### 3. Buenas prácticas\n",
      "\n",
      "| Buenas prácticas | Por qué son importantes |\n",
      "|------------------|------------------------|\n",
      "| **Versionar el esquema** | Evita romper downstream cuando cambian columnas. |\n",
      "| **Separar datos brutos y procesados** | Permite volver atrás y rehacer transformaciones sin perder datos. |\n",
      "| **Automatizar flujos** | Airflow, Prefect, Dagster o cron minimizan errores manuales. |\n",
      "| **Monitoreo y alertas** | Detecta fallos rápidamente y evita pérdida de datos. |\n",
      "| **Documentar** | Los equipos entienden flujos y dependencias; facilita onboarding. |\n",
      "| **Seguridad** | Control de acceso, cifrado en reposo y en tránsito. |\n",
      "\n",
      "### 4. Un ejemplo simple en Python (ETL con Pandas)\n",
      "\n",
      "Supongamos que recibimos un archivo CSV diario con ventas y queremos cargarlo en una tabla de Snowflake.\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "from sqlalchemy import create_engine\n",
      "import snowflake.connector\n",
      "from snowflake.sqlalchemy import URL\n",
      "\n",
      "# 1. Leer datos brutos\n",
      "df = pd.read_csv('ventas_2025-09-13.csv')\n",
      "\n",
      "# 2. Transformar: convertir fechas y limpiar nulos\n",
      "df['fecha'] = pd.to_datetime(df['fecha'])\n",
      "df = df.dropna(subset=['producto_id', 'monto'])\n",
      "\n",
      "# 3. Enriquecer: agregar una columna de descuento calculado\n",
      "df['descuento'] = df['monto'] * 0.05\n",
      "\n",
      "# 4. Conectar a Snowflake\n",
      "engine = create_engine(URL(\n",
      "    account='mi-cuenta.snowflakecomputing.com',\n",
      "    user='usuario',\n",
      "    password='contraseña',\n",
      "    database='DW',\n",
      "    schema='VENTAS'\n",
      "))\n",
      "\n",
      "# 5. Cargar en la tabla (crea tabla si no existe)\n",
      "df.to_sql('ventas_diarias', con=engine, if_exists='append', index=False)\n",
      "```\n",
      "\n",
      "> **Tip**: Si el volumen crece, reemplaza `pandas` por `PySpark` y utiliza `write.jdbc` para un rendimiento mayor.\n",
      "\n",
      "### 5. Cuando trabajas con Big Data\n",
      "\n",
      "| Herramienta | Uso típico |\n",
      "|-------------|------------|\n",
      "| **Apache Spark** | Transformación distribuida en batch y streaming. |\n",
      "| **Apache Beam** | Unificar batch y streaming con runners como Flink o Google Dataflow. |\n",
      "| **Delta Lake / Iceberg** | Versionado de tablas, ACID en big data. |\n",
      "| **dbt** | Transformaciones SQL en el warehouse; versionado con Git. |\n",
      "\n",
      "---\n",
      "\n",
      "## Resumen\n",
      "\n",
      "Data Engineering es la base que permite que los datos lleguen de forma estructurada y confiable a quien los necesite. Con una buena arquitectura de ingestión, almacenamiento y transformación, y siguiendo buenas prácticas de versionado, automatización y monitoreo, se logra un flujo de datos robusto y escalable que potencia decisiones basadas en datos.\n"
     ]
    }
   ],
   "source": [
    "pregunta = \"Que es Data Engineering?\"\n",
    "respuesta = llm(pregunta)\n",
    "respueta_bbd = bbdd(pregunta, respuesta)\n",
    "\n",
    "if respueta_bbd == \"ok\":\n",
    "    print(pregunta)\n",
    "    print(\"___________\")\n",
    "    print(respuesta)\n",
    "else:\n",
    "    print(\"Error en la pregunta\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
